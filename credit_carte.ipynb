{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e64c2ab",
   "metadata": {},
   "source": [
    "#Cas pratique de Machine Learning avec Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3f7d75",
   "metadata": {},
   "source": [
    "Il s'agit pour nous dans cet exercice de travailler avec une serie de données issues de cartes de credits,\n",
    "et de predire grace elles si un individu est un bon creantier (1) ou non (0) en utilisant trois modèles de machine\n",
    "learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3182dee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialiser Spark et importer les librairies requis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e6b6774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Trouver\" PySpark et demarrer une session Spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dceb703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importer les librairies requis\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2ea25c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55582d2a",
   "metadata": {},
   "source": [
    "1- Chargement des données\n",
    "\n",
    "Les données sont ressemblées dans un fichier texte ccdefault.csv (dans le dossier data)\n",
    "dont la première ligne contient le nom des colonnes,\n",
    "nous les lirons dans un dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f5e6dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lire le fichier csv\n",
    "data = spark.read.csv(\"./data/ccdefault.csv\", header = True, inferSchema = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c64f477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'observations: 30000\n",
      "Nombre de colonnes: 25\n",
      "Nom des colonnes: ['ID', 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 'DEFAULT']\n"
     ]
    }
   ],
   "source": [
    "#Presentation sommaire\n",
    "print(\"Nombre d'observations:\", data.count())\n",
    "print(\"Nombre de colonnes:\", len(data.schema.names))\n",
    "print(\"Nom des colonnes:\", data.schema.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92f28a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+---------+--------+---+-----+-----+---------+---------+-------+\n",
      "| ID|LIMIT_BAL|SEX|EDUCATION|MARRIAGE|AGE|PAY_0|PAY_0|BILL_AMT1|BILL_AMT2|DEFAULT|\n",
      "+---+---------+---+---------+--------+---+-----+-----+---------+---------+-------+\n",
      "|  1|    20000|  2|        2|       1| 24|    2|    2|     3913|     3102|      1|\n",
      "|  2|   120000|  2|        2|       2| 26|   -1|   -1|     2682|     1725|      1|\n",
      "|  3|    90000|  2|        2|       2| 34|    0|    0|    29239|    14027|      0|\n",
      "|  4|    50000|  2|        2|       1| 37|    0|    0|    46990|    48233|      0|\n",
      "|  5|    50000|  1|        2|       1| 57|   -1|   -1|     8617|     5670|      0|\n",
      "+---+---------+---+---------+--------+---+-----+-----+---------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.select(\"ID\", \"LIMIT_BAL\", \"SEX\", \"EDUCATION\", \"MARRIAGE\", \"AGE\", \"PAY_0\", \"PAY_0\", \n",
    "                  \"BILL_AMT1\", \"BILL_AMT2\", \"DEFAULT\").show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07b26916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de clients fiable Default == 1: 6636\n",
      "Nombre de clients non fiable Default == 0: 23364\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombre de clients fiable Default == 1:\", data.filter(data.DEFAULT == 1).count())\n",
    "print(\"Nombre de clients non fiable Default == 0:\", data.filter(data.DEFAULT == 0).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b01549",
   "metadata": {},
   "source": [
    "On contaste qu'une très grande majorité de client ne sont pas fiables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d73ca1f",
   "metadata": {},
   "source": [
    "#Mise en oeuvre de la pipeline\n",
    "\n",
    "Les données seront scindées en deux parties (données d'entrainement et données test).\n",
    "Les données numeriques seront normalisées.\n",
    "Cependant les données des champs allant de PAY_0 à PAY_6 sont en realité des données labelisées et devraient être \n",
    "factorisées; mais dans notre exercices cela n'est pas vraiment important \n",
    "dans cet exercice \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35226c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/10 19:51:14 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "#Abondon de l'ID des colonnes\n",
    "data = data.select(data.schema.names[1:])\n",
    "\n",
    "#Scinder le jeux de données en deux parties (entrainement et test)\n",
    "splits = data.randomSplit([0.75, 0.25])\n",
    "data_train = splits[0]\n",
    "data_test = splits[1]\n",
    "\n",
    "#Convertir les champs qualitatifs (SEX, EDUCATION, MARRIAGE)\n",
    "categorical_features = data.schema.names[1:4]\n",
    "catVect = VectorAssembler(inputCols = categorical_features, outputCol = \"catFeatures\")\n",
    "catIdx = VectorIndexer(inputCol = catVect.getOutputCol(), outputCol = \"idxCatFeatures\")\n",
    "\n",
    "#Normaliser les champs quantitatifs\n",
    "numerical_features = data.schema.names[0:1] + data.schema.names[4:]\n",
    "numVect = VectorAssembler(inputCols = numerical_features, outputCol = \"numFeatures\")\n",
    "minMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol = \"normFeatures\")\n",
    "\n",
    "# Definir le pipeline \n",
    "featVect = VectorAssembler(inputCols=[\"idxCatFeatures\", \"normFeatures\"], outputCol = \"features\")\n",
    "pipeline = Pipeline(stages = [catVect, catIdx, numVect, minMax, featVect])\n",
    "pipeline_object = pipeline.fit(data_train)\n",
    "\n",
    "#Exécuter les données d'entrainement et de test à travers le pipeline\n",
    "data_train = pipeline_object.transform(data_train).select(\"features\", col(\"DEFAULT\").alias(\"label\"))\n",
    "data_test = pipeline_object.transform(data_test).select(\"features\", col(\"DEFAULT\").alias(\"label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e02d200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[0.0,1.0,1.0,0.0,...|    1|\n",
      "|[0.0,1.0,1.0,0.0,...|    0|\n",
      "|[0.0,1.0,1.0,0.0,...|    0|\n",
      "|[0.0,1.0,1.0,0.0,...|    0|\n",
      "|[0.0,1.0,1.0,0.0,...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "Nombre de données pour l'entrainement /le test: 22458 / 7542\n"
     ]
    }
   ],
   "source": [
    "print(data_train.show(5))\n",
    "print(\"Nombre de données pour l'entrainement /le test:\", data_train.count(), \"/\", data_test.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b28e5eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTRAINEMENT ET EVALUATION DES MODELES DE CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363a180",
   "metadata": {},
   "source": [
    "Définir les fonctions des métriques d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c9ca57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol = \"label\", predictionCol = \"prediction\", metricName = \"accuracy\")\n",
    "precision = MulticlassClassificationEvaluator(\n",
    "    labelCol = \"label\", predictionCol = \"prediction\", metricName = \"weightedPrecision\")\n",
    "recall = MulticlassClassificationEvaluator(\n",
    "    labelCol = \"label\", predictionCol = \"prediction\", metricName = \"weightedRecall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75b6aec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance: 0.9801\n",
      "Précision pondérée: 0.9805\n",
      "Rappel pondéré: 0.9801\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression\n",
    "\n",
    "logit = LogisticRegression(labelCol = \"label\", featuresCol = \"features\", maxIter = 20, regParam = 0.2)\n",
    "model = logit.fit(data_train)\n",
    "predictions_df = model.transform(data_test)\n",
    "\n",
    "\n",
    "print(\"performance: {:.4}\".format(accuracy.evaluate(predictions_df)))\n",
    "print(\"Précision pondérée: {:.4}\".format(precision.evaluate(predictions_df)))\n",
    "print(\"Rappel pondéré: {:.4}\".format(recall.evaluate(predictions_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a343ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision: 1.0\n",
      "Précision pondérée: 1.0\n",
      "Rappel pondéré: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Decision tree\n",
    "tree = DecisionTreeClassifier(labelCol = \"label\", featuresCol = \"features\", maxDepth = 4, maxBins = 32, \n",
    "                              minInstancesPerNode = 1, minInfoGain = 0.0, impurity = \"gini\", seed = 123)\n",
    "model = tree.fit(data_train)\n",
    "predictions_df = model.transform(data_test)\n",
    "\n",
    "print(\"performance: {:.4}\".format(accuracy.evaluate(predictions_df)))\n",
    "print(\"Précision pondérée: {:.4}\".format(precision.evaluate(predictions_df)))\n",
    "print(\"Rappel pondéré: {:.4}\".format(recall.evaluate(predictions_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60fa61e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance: 1.0\n",
      "Précision pondérée: 1.0\n",
      "Rappel pondéré: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Random forest\n",
    "rf = RandomForestClassifier(labelCol = \"label\", featuresCol = \"features\", maxDepth = 4, maxBins = 32, \n",
    "                            minInstancesPerNode = 1, minInfoGain=0.0, impurity = \"gini\", numTrees = 10, seed = 123) \n",
    "model = rf.fit(data_train)\n",
    "predictions_df = model.transform(data_test)\n",
    "\n",
    "print(\"performance: {:.4}\".format(accuracy.evaluate(predictions_df)))\n",
    "print(\"Précision pondérée: {:.4}\".format(precision.evaluate(predictions_df)))\n",
    "print(\"Rappel pondéré: {:.4}\".format(recall.evaluate(predictions_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "277b869d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vrais positifs: 1665\n",
      "faux positifs: 0\n",
      "vrais negatifs: 5877\n",
      "faux negatifs: 0\n",
      "performance: 1.0\n",
      "Precision: 1.0\n",
      "Rappel: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Calculer les métriques (non pondérées) manuellement\n",
    "tp = int(predictions_df.filter(\"prediction == 1.0 AND label == 1\").count())\n",
    "fp = int(predictions_df.filter(\"prediction == 1.0 AND label == 0\").count())\n",
    "tn = int(predictions_df.filter(\"prediction == 0.0 AND label == 0\").count())\n",
    "fn = int(predictions_df.filter(\"prediction == 0.0 AND label == 1\").count())\n",
    "\n",
    "print(\"vrais positifs:\", tp)\n",
    "print(\"faux positifs:\", fp)\n",
    "print(\"vrais negatifs:\", tn)\n",
    "print(\"faux negatifs:\", fn)\n",
    "\n",
    "print(\"performance: {:.4}\".format((tp+tn)/(tp+fp+tn+fn)))\n",
    "print(\"Precision: {:.4}\".format((tp)/(tp+fp)))\n",
    "print(\"Rappel: {:.4}\".format((tp)/(tp+fn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fdc4ac",
   "metadata": {},
   "source": [
    "Au vu des performances mesurées on peut sans contexte dire \"Random forest\" et le \"decision tree\" sont indiquées pour \n",
    "cet apprentissage, même si la \"logistic regression\" donne aussi de bons résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1f5832",
   "metadata": {},
   "source": [
    "Avec ces données, on peut essayer de predire le montant idéal\n",
    "c'est à dire assez proche de ce que le client souhaite tout en etant certain que celui rembourse. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d021622",
   "metadata": {},
   "source": [
    "On avait aussi la possibilité de faire cet exercice avec la librairie sklearn,\n",
    "il est curieux de savoir quels resultats l'on aurait obtenus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebafbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
